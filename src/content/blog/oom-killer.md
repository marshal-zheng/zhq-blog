---
author: ZHQ
pubDatetime: 2025-06-11T11:45:00+08:00
title: '记一次 Linux OOM Killer 引发的服务中断排查全流程'
featured: false
draft: false
tags:
  - 'daily digest'
description: '2025-05-10 信息摘要信息'
---

最近在对 `make/plane` 这个项目做二次开发，不得不说，这个项目确实有点分量。手里的 MacBook Pro 内存吃紧，在本地跑起来之后风扇狂转，开发体验非常卡顿。为了提升效率，干脆把开发环境挪到了公司的一台大内存服务器上，用远程 IDE 来写代码。

刚开始一切都很丝滑，但好景不长，使用了一段时间 `next-server` 进程总是莫名其妙地自己挂掉。查应用日志也看不出个所以然。经过一番排查，最终定位是服务器内存耗尽导致的。

这类问题其实在开发和运维中挺常见的，所以借着这个机会，把整个排查和解决的流程完整地记录下来，希望能为遇到类似问题的朋友提供一个清晰的参考。

### 第一步：从 dmesg 开始，找到第一条线索

当一个进程在没有任何应用层错误日志的情况下异常退出时，我们首先应该怀疑是不是被系统层面“干预”了。Linux 内核日志是排查这类问题的首选之地。

通过 `dmesg` 命令，我们可以查看内核环形缓冲区的信息：

```bash
# 使用 grep 筛选和 OOM 或 kill 相关的信息
dmesg | grep -i "Out of memory"
# 或者更宽泛地
dmesg | grep -i "kill"
```

执行命令后，我果然找到了关键日志：

```log
[12345.67890] Out of memory: Killed process 3281050 (next-server) total-vm:73269092kB, anon-rss:5899896kB, file-rss:0kB, shmem-rss:0kB
```

这条日志信息量很大，直接告诉了我们：

*   **发生了什么**：`Out of memory`，内存不足。
*   **系统的处置**：`Killed process 3281050 (next-server)`，系统启动了 OOM Killer 机制，杀掉了 PID 为 `3281050`、名为 `next-server` 的进程。
*   **为何是它**：日志后面的 `total-vm` 和 `anon-rss` 等指标说明了该进程在被终止时占用了大量内存。

到这里，问题基本定性了：**服务是被系统的 OOM Killer 终止的**。OOM Killer 是内核在系统内存极度短缺时，为避免整个系统崩溃而采取的一种自我保护措施，它会选择一个“得分”最高的进程（通常是占用内存最多的）来终止，以释放内存。

### 第二步：检查事发时的系统资源快照

确认了是 OOM，下一步就是了解当时系统的整体负载情况，为什么会触发 OOM。

首先，检查内存和交换分区的使用情况：

```bash
free -h
```

这个命令可以快速了解物理内存（Mem）和交换空间（Swap）的总量、已用量和可用量。如果 `available` 的值非常低，同时 `swap` 已被大量使用甚至用尽，就说明系统内存压力巨大。

接着，查看当前系统中消耗内存最多的进程：

```bash
ps -eo user,pid,%mem,%cpu,cmd --sort=-%mem | head -n 10
```

虽然导致 OOM 的元凶（我的 `next-server`）已经被 kill，但这个命令可以帮助我们看到当前还有哪些服务是内存消耗大户，对系统的整体资源分布有个概念。

### 分析根源：常见的内存失控诱因

定位到具体进程后，就需要深入分析内存为何会失控。结合我的情况和过往经验，常见的原因有以下几类：

1.  **内存泄漏**：这是最普遍的原因。应用代码中存在无法被垃圾回收机制（GC）释放的对象，导致内存随时间推移只增不减，最终耗尽系统资源。
2.  **瞬间业务高峰**：短时间内有大量请求涌入，或者某个后台任务处理了超预期的数据量，导致应用瞬间申请大量内存，超出了系统承受能力。
3.  **无限制的进程**：这是我这次犯的错误。在服务器上启动 `next-server` 时，没有为其设置任何内存使用上限。这导致它作为一个开发中的服务，可以无节制地“吞噬”宿主机的内存，最终影响到系统稳定性。
4.  **物理资源不足**：宿主机本身的物理内存就不够，或者 Swap 空间设置得过小，无法应对业务负载。

对于我这次的问题，主要原因就是第三点——没有对开发进程做资源隔离和限制。

### 解决与预防：从根源上杜绝后患

找到根源后，修复起来就很有针对性了。我的目标不仅是让服务恢复，更是要建立长效机制，防止问题复现。

#### 核心措施：为进程设置内存限制

这是最直接有效的手段。对应用进程的资源使用进行明确限制，是保证多租户环境（比如一台服务器上跑多个服务）稳定性的基石。

*   **容器化环境 (Docker)**：这是最佳实践。通过 `docker run` 的 `--memory` (或 `-m`) 参数可以轻松限制。例如：`docker run -m 2g ...`，这样容器最多只能使用 2GB 物理内存，一旦超出，OOM Killer 会先在容器内部起作用，不会影响宿主机。
*   **非容器化环境 (如 Node.js)**：可以直接在启动命令里加参数。对于 Node.js，可以使用 `--max-old-space-size` 来限制 V8 引擎的老生代堆内存大小。例如：`node --max-old-space-size=2048 index.js`。
*   **JVM 应用**：则使用大家熟知的 `-Xms` 和 `-Xmx` 参数。

#### 代码层面的内存优化

如果排查发现是内存泄漏，那就必须深入代码了。

*   **使用分析工具**：
    *   **Java**: 使用 `jmap` 导出堆快照，再用 MAT 或 JProfiler 等工具进行分析。
    *   **Node.js**: `clinic.js` 套件或 `heapdump` 模块是非常好的分析工具。
    *   **Go/C++**: `pprof` 或 `Valgrind` 等。
*   **关注重点**：重点审查全局集合（Map, List）、缓存、定时任务、文件句柄、数据库连接等资源，确保它们被正确、及时地释放。

#### 辅助手段：调整 Swap 空间

如果物理内存确实紧张，临时增加 Swap 空间可以作为一个缓冲，避免系统过于脆弱。

```bash
# 创建一个 4GB 大小的 swap 文件
sudo dd if=/dev/zero of=/swapfile bs=1M count=4096
# 格式化为 swap
sudo mkswap /swapfile
# 启用 swap
sudo swapon /swapfile
# 设置开机自动挂载
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
```
**注意**：Swap 的性能远低于物理内存。过度依赖 Swap 会导致应用性能急剧下降，这只是一个权宜之计，治本还需优化应用或增加物理内存。

#### 长效机制：建立完善的监控与告警

查询了GPT，给出了下面建议: 为了变被动为主动，需要建立监控。

*   **监控指标**：系统总内存/Swap使用率、**单进程内存使用率**、GC 活动情况等。
*   **告警规则**：设置合理的阈值，例如，当内存使用率超过 85% 并持续 5 分钟，就触发告警。这样我们就有机会在 OOM Killer 出手前介入。

### 总结

这次由 `make/plane` 开发环境引发的 OOM 问题，最终通过一套标准的排查流程得到了解决。总结下来，核心思路就是：

1.  **确认死因**：从 `dmesg` 入手，确认是 OOM Killer 所为。
2.  **评估现场**：用 `free` 和 `ps` 等命令了解事发时系统资源状况。
3.  **分析根源**：判断是代码泄漏、流量冲击，还是配置不当。
4.  **根治与预防**：**核心是限制进程内存**，其次是优化代码和合理扩容，最后通过**监控告警**建立长效保障。
